//package antlr
//
//import (
//  "fmt"
//  "strconv"
//)
//
//// A lexer is recognizer that draws input symbols from a character stream.
////  lexer grammars result in a subclass of this object. A Lexer object
////  uses simplified Match() and error recovery mechanisms in the i32erest
////  of speed.
/////
//
//pub trait Lexer {
//  TokenSource
//  Recognizer
//
//  Emit() Token
//
//  set_channel(int)
//  push_mode(int)
//  pop_mode() i32
//  set_type(int)
//  set_mode(int)
//}
//
//pub struct BaseLexer {
//  *BaseRecognizer
//
//  Interpreter:         ILexerATNSimulator
//  TokenStartCharIndex: i32
//  TokenStartLine:      i32
//  TokenStartColumn:    i32
//  ActionType:          i32
//  Virt                Lexer // The most derived lexer implementation. Allows virtual method calls.
//
//  input:                  CharStream
//  factory:                TokenFactory
//  token_factory_source_pair: *TokenSourceCharStreamPair
//  token:                  Token
//  hit_e_o_f:                 bool
//  channel:                i32
//  thetype:                i32
//  mode_stack:              IntStack
//  mode:                   i32
//  text:                   &str
//}
//
//impl BaseLexer {ยง//  fn new(&self, input: CharStream) -> *BaseLexer {
//
//  let lexer = new(BaseLexer);
//
//  lexer.base_recognizer = NewBaseRecognizer()
//
//  lexer.input = input
//  lexer.factory = CommonTokenFactoryDEFAULT
//  lexer.token_factory_source_pair = &TokenSourceCharStreamPair{lexer, input}
//
//  lexer.Virt = lexer
//
//  lexer.Interpreter = nil // child classes must populate it
//
//  // The goal of all lexer rules/methods is to create a token object.
//  // l is an instance variable as multiple rules may collaborate to
//  // create a single token. NextToken will return l object after
//  // Matching lexer rule(s). If you subclass to allow multiple token
//  // emissions, then set l to the last token to be Matched or
//  // something nonnil so that the auto token emit mechanism will not
//  // emit another token.
//  lexer.token = nil
//
//  // What character index in the stream did the current token start at?
//  // Needed, for example, to get the text for current token. Set at
//  // the start of NextToken.
//  lexer.token_start_char_index = -1
//
//  // The line on which the first character of the token resides///
//  lexer.token_start_line = -1
//
//  // The character position of first character within the line///
//  lexer.token_start_column = -1
//
//  // Once we see EOF on char stream, next token will be EOF.
//  // If you have DONE : EOF  then you see DONE EOF.
//  lexer.hit_e_o_f = false
//
//  // The channel number for the current token///
//  lexer.channel = TokenDefaultChannel
//
//  // The token type for the current token///
//  lexer.thetype = TokenInvalidType
//
//  lexer.mode_stack = make([]int, 0)
//  lexer.mode = LexerDefaultMode
//
//  // You can set the text for the current token to override what is in
//  // the input char buffer. Use set_text() or can set l instance var.
//  // /
//  lexer.text = ""
//
//  return lexer
//}
//
//const (
//  LexerDefaultMode = 0
//  LexerMore        = -2
//  LexerSkip        = -3
//)
//
//const (
//  LexerDefaultTokenChannel = TokenDefaultChannel
//  LexerHidden              = TokenHiddenChannel
//  LexerMinCharValue        = '\u0000'
//  LexerMaxCharValue        = '\uFFFE'
//)
//
//fn reset(&self, ) {
//  // wack Lexer state variables
//  if self.input != nil {
//    self.input.Seek(0) // rewind the input
//  }
//  self.token = nil
//  self.thetype = TokenInvalidType
//  self.channel = TokenDefaultChannel
//  self.token_start_char_index = -1
//  self.token_start_column = -1
//  self.token_start_line = -1
//  self.text = ""
//
//  self.hit_e_o_f = false
//  self.mode = LexerDefaultMode
//  self.mode_stack = make([]int, 0)
//
//  self.Interpreter.reset()
//}
//
//pub fn interpreter(&self, ) -> ILexerATNSimulator {
//  return self.Interpreter
//}
//
//pub fn input_stream(&self, ) -> CharStream {
//  return self.input
//}
//
//pub fn source_name(&self, ) -> &str {
//  return self.grammar_file_name
//}
//
//fn set_channel(&self, v: i32) {
//  self.channel = v
//}
//
//pub fn token_factory(&self, ) -> TokenFactory {
//  return self.factory
//}
//
//fn set_token_factory(&self, f: TokenFactory) {
//  self.factory = f
//}
//
//pub fn (&self, ret: i32) {
//  defer func() {
//    if let e = recover(); e != nil {;
//      if re, let ok = e.(RecognitionException); ok {;
//        self.notify_listeners(re) // Report error
//        self.Recover(re)
//        ret = LexerSkip // default
//      }
//    }
//  }()
//
//  return self.Interpreter.Match(self.input, self.mode)
//}
//
//// Return a token from l source self.e., Match a token on the char stream.
//pub fn next_token(&self, ) -> Token {
//  if self.input == nil {
//    panic("NextToken requires a non-nil input stream.")
//  }
//
//  let token_start_marker = self.input.Mark();
//
//  // previously in finally block
//  defer func() {
//    // make sure we release marker after Match or
//    // unbuffered char stream will keep buffering
//    self.input.Release(tokenStartMarker)
//  }()
//
//  for {
//    if self.hit_e_o_f {
//      self.emit_e_o_f()
//      return self.token
//    }
//    self.token = nil
//    self.channel = TokenDefaultChannel
//    self.token_start_char_index = self.input.Index()
//    self.token_start_column = self.Interpreter.char_position_in_line()
//    self.token_start_line = self.Interpreter.line()
//    self.text = ""
//    let continue_outer = false;
//    for {
//      self.thetype = TokenInvalidType
//      let ttype = LexerSkip;
//
//      ttype = self.safe_match()
//
//      if self.input.l_a(1) == TokenEOF {
//        self.hit_e_o_f = true
//      }
//      if self.thetype == TokenInvalidType {
//        self.thetype = ttype
//      }
//      if self.thetype == LexerSkip {
//        continue_outer = true
//        break
//      }
//      if self.thetype != LexerMore {
//        break
//      }
//    }
//
//    if continue_outer {
//      continue
//    }
//    if self.token == nil {
//      self.Virt.Emit()
//    }
//    return self.token
//  }
//
//  return nil
//}
//
//// Instruct the lexer to Skip creating a token for current lexer rule
//// and look for another token. NextToken() knows to keep looking when
//// a lexer rule finishes with token set to SKIPTOKEN. Recall that
//// if token==nil at end of any token rule, it creates one for you
//// and emits it.
//// /
//pub fn Skip(&self, ) {
//  self.thetype = LexerSkip
//}
//
//pub fn More(&self, ) {
//  self.thetype = LexerMore
//}
//
//fn set_mode(&self, m: i32) {
//  self.mode = m
//}
//
//fn push_mode(&self, m: i32) {
//  if LexerATNSimulatorDebug {
//    fmt.Println("pushMode " + strconv.Itoa(m))
//  }
//  self.mode_stack.Push(self.mode)
//  self.mode = m
//}
//
//fn pop_mode(&self, ) -> i32 {
//  if len(self.mode_stack) == 0 {
//    panic("Empty Stack")
//  }
//  if LexerATNSimulatorDebug {
//    fmt.Println("popMode back to " + fmt.Sprint(self.mode_stack[0:len(self.mode_stack)-1]))
//  }
//  i, _let  = self.mode_stack.Pop();
//  self.mode = i
//  return self.mode
//}
//
//fn input_stream(&self, ) -> CharStream {
//  return self.input
//}
//
//fn set_input_stream(&self, input: CharStream) {
//  self.input = nil
//  self.token_factory_source_pair = &TokenSourceCharStreamPair{b, self.input}
//  self.reset()
//  self.input = input
//  self.token_factory_source_pair = &TokenSourceCharStreamPair{b, self.input}
//}
//
//// By default does not support multiple emits per NextToken invocation
//// for efficiency reasons. Subclass and override l method, NextToken,
//// and GetToken (to push tokens i32o a list and pull from that list
//// rather than a single variable as l implementation does).
//// /
//pub fn emit_token(&self, token: Token) {
//  self.token = token
//}
//
//// The standard method called to automatically emit a token at the
//// outermost lexical rule. The token object should point i32o the
//// char buffer start..stop. If there is a text override in 'text',
//// use that to set the token's text. Override l method to emit
//// custom Token objects or provide a Newfactory.
//// /
//pub fn Emit(&self, ) -> Token {
//  let t = self.factory.Create(self.token_factory_source_pair, self.thetype, self.text, self.channel, self.token_start_char_index, self.char_index()-1, self.token_start_line, self.token_start_column);
//  self.emit_token(t)
//  return t
//}
//
//pub fn emit_e_o_f(&self, ) -> Token {
//  let cpos = self.char_position_in_line();
//  let lpos = self.line();
//  let eof = self.factory.Create(self.token_factory_source_pair, TokenEOF, "", TokenDefaultChannel, self.input.Index(), self.input.Index()-1, lpos, cpos);
//  self.emit_token(eof)
//  return eof
//}
//
//pub fn char_position_in_line(&self, ) -> i32 {
//  return self.Interpreter.char_position_in_line()
//}
//
//pub fn line(&self, ) -> i32 {
//  return self.Interpreter.line()
//}
//
//pub fn type(&self, ) -> i32 {
//  return self.thetype
//}
//
//fn set_type(&self, t: i32) {
//  self.thetype = t
//}
//
//// What is the index of the current character of lookahead?///
//pub fn char_index(&self, ) -> i32 {
//  return self.input.Index()
//}
//
//// Return the text Matched so far for the current token or any text override.
////Set the complete text of l token it wipes any previous changes to the text.
//pub fn text(&self, ) -> &str {
//  if self.text != "" {
//    return self.text
//  }
//
//  return self.Interpreter.text(self.input)
//}
//
//pub fn set_text(&self, text: &str) {
//  self.text = text
//}
//
//pub fn a_t_n(&self, ) -> *ATN {
//  return self.Interpreter.a_t_n()
//}
//
//// Return a list of all Token objects in input char stream.
//// Forces load of all tokens. Does not include EOF token.
//// /
//fn all_tokens(&self, ) -> []Token {
//  let vl = self.Virt;
//  let tokens = make([]Token, 0);
//  let t = vl.next_token();
//  for self.token_type() != TokenEOF {
//    tokens = append(tokens, t)
//    t = vl.next_token()
//  }
//  return tokens
//}
//
//fn notify_listeners(&self, e: RecognitionException) {
//  let start = self.token_start_char_index;
//  let stop = self.input.Index();
//  let text = self.input.text_from_interval(NewInterval(start, stop));
//  let msg = "token recognition error at: '" + text + "'";
//  let listener = self.error_listener_dispatch();
//  listener.syntax_error(b, nil, self.token_start_line, self.token_start_column, msg, e)
//}
//
//fn error_display_for_char(&self, c: rune) -> &str {
//  if c == TokenEOF {
//    return "<EOF>"
//  } else if c == '\n' {
//    return "\\n"
//  } else if c == '\t' {
//    return "\\t"
//  } else if c == '\r' {
//    return "\\r"
//  } else {
//    return &str(c)
//  }
//}
//
//fn char_error_display(&self, c: rune) -> &str {
//  return "'" + self.error_display_for_char(c) + "'"
//}
//
//// Lexers can normally Match any char in it's vocabulary after Matching
//// a token, so do the easy thing and just kill a character and hope
//// it all works out. You can instead use the rule invocation stack
//// to do sophisticated error recovery if you are in a fragment rule.
//// /
//pub fn Recover(&self, re: RecognitionException) {
//  if self.input.l_a(1) != TokenEOF {
//    if _, let ok = re.(*LexerNoViableAltException); ok {;
//      // Skip a char and try again
//      self.Interpreter.Consume(self.input)
//    } else {
//      // TODO: Do we lose character or line position information?
//      self.input.Consume()
//    }
//  }
//}
