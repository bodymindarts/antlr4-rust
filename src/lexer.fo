//package antlr
//
//import (
//  "fmt"
//  "strconv"
//)
//
//// A lexer is recognizer that draws input symbols from a character stream.
////  lexer grammars result in a subclass of this object. A Lexer object
////  uses simplified Match() and error recovery mechanisms in the i32erest
////  of speed.
/////
//
//pub trait Lexer {
//  TokenSource
//  Recognizer
//
//  Emit() Token
//
//  set_channel(int)
//  push_mode(int)
//  pop_mode() i32
//  set_type(int)
//  set_mode(int)
//}
//
//pub struct BaseLexer {
//  *BaseRecognizer
//
//  Interpreter:         ILexerATNSimulator
//  TokenStartCharIndex: i32
//  TokenStartLine:      i32
//  TokenStartColumn:    i32
//  ActionType:          i32
//  Virt                Lexer // The most derived lexer implementation. Allows virtual method calls.
//
//  input:                  CharStream
//  factory:                TokenFactory
//  token_factory_source_pair: *TokenSourceCharStreamPair
//  token:                  Token
//  hit_e_o_f:                 bool
//  channel:                i32
//  thetype:                i32
//  mode_stack:              IntStack
//  mode:                   i32
//  text:                   &str
//}
//
//impl BaseLexer {ยง//  pub fn new(&self, input: CharStream) -> *BaseLexer {
//
//  let lexer = new(BaseLexer);
//
//  lexer.BaseRecognizer = NewBaseRecognizer()
//
//  lexer.input = input
//  lexer.factory = CommonTokenFactoryDEFAULT
//  lexer.token_factory_source_pair = &TokenSourceCharStreamPair{lexer, input}
//
//  lexer.Virt = lexer
//
//  lexer.Interpreter = nil // child classes must populate it
//
//  // The goal of all lexer rules/methods is to create a token object.
//  // l is an instance variable as multiple rules may collaborate to
//  // create a single token. NextToken will return l object after
//  // Matching lexer rule(s). If you subclass to allow multiple token
//  // emissions, then set l to the last token to be Matched or
//  // something nonnil so that the auto token emit mechanism will not
//  // emit another token.
//  lexer.token = nil
//
//  // What character index in the stream did the current token start at?
//  // Needed, for example, to get the text for current token. Set at
//  // the start of NextToken.
//  lexer.TokenStartCharIndex = -1
//
//  // The line on which the first character of the token resides///
//  lexer.TokenStartLine = -1
//
//  // The character position of first character within the line///
//  lexer.TokenStartColumn = -1
//
//  // Once we see EOF on char stream, next token will be EOF.
//  // If you have DONE : EOF  then you see DONE EOF.
//  lexer.hit_e_o_f = false
//
//  // The channel number for the current token///
//  lexer.channel = TokenDefaultChannel
//
//  // The token type for the current token///
//  lexer.thetype = TokenInvalidType
//
//  lexer.mode_stack = make([]int, 0)
//  lexer.mode = LexerDefaultMode
//
//  // You can set the text for the current token to override what is in
//  // the input char buffer. Use set_text() or can set l instance var.
//  // /
//  lexer.text = ""
//
//  return lexer
//}
//
//const (
//  LexerDefaultMode = 0
//  LexerMore        = -2
//  LexerSkip        = -3
//)
//
//const (
//  LexerDefaultTokenChannel = TokenDefaultChannel
//  LexerHidden              = TokenHiddenChannel
//  LexerMinCharValue        = '\u0000'
//  LexerMaxCharValue        = '\uFFFE'
//)
//
//pub fn reset(&self, ) {
//  // wack Lexer state variables
//  if b.input != nil {
//    b.input.Seek(0) // rewind the input
//  }
//  b.token = nil
//  b.thetype = TokenInvalidType
//  b.channel = TokenDefaultChannel
//  b.TokenStartCharIndex = -1
//  b.TokenStartColumn = -1
//  b.TokenStartLine = -1
//  b.text = ""
//
//  b.hit_e_o_f = false
//  b.mode = LexerDefaultMode
//  b.mode_stack = make([]int, 0)
//
//  b.Interpreter.reset()
//}
//
//pub fn GetInterpreter(&self, ) -> ILexerATNSimulator {
//  return b.Interpreter
//}
//
//pub fn GetInputStream(&self, ) -> CharStream {
//  return b.input
//}
//
//pub fn GetSourceName(&self, ) -> &str {
//  return b.GrammarFileName
//}
//
//pub fn set_channel(&self, v: i32) {
//  b.channel = v
//}
//
//pub fn GetTokenFactory(&self, ) -> TokenFactory {
//  return b.factory
//}
//
//pub fn set_token_factory(&self, f: TokenFactory) {
//  b.factory = f
//}
//
//pub fn (&self, ret: i32) {
//  defer func() {
//    if let e = recover(); e != nil {;
//      if re, let ok = e.(RecognitionException); ok {;
//        b.notify_listeners(re) // Report error
//        b.Recover(re)
//        ret = LexerSkip // default
//      }
//    }
//  }()
//
//  return b.Interpreter.Match(b.input, b.mode)
//}
//
//// Return a token from l source i.e., Match a token on the char stream.
//pub fn NextToken(&self, ) -> Token {
//  if b.input == nil {
//    panic("NextToken requires a non-nil input stream.")
//  }
//
//  let token_start_marker = b.input.Mark();
//
//  // previously in finally block
//  defer func() {
//    // make sure we release marker after Match or
//    // unbuffered char stream will keep buffering
//    b.input.Release(tokenStartMarker)
//  }()
//
//  for {
//    if b.hit_e_o_f {
//      b.EmitEOF()
//      return b.token
//    }
//    b.token = nil
//    b.channel = TokenDefaultChannel
//    b.TokenStartCharIndex = b.input.Index()
//    b.TokenStartColumn = b.Interpreter.GetCharPositionInLine()
//    b.TokenStartLine = b.Interpreter.GetLine()
//    b.text = ""
//    let continue_outer = false;
//    for {
//      b.thetype = TokenInvalidType
//      let ttype = LexerSkip;
//
//      ttype = b.safe_match()
//
//      if b.input.LA(1) == TokenEOF {
//        b.hit_e_o_f = true
//      }
//      if b.thetype == TokenInvalidType {
//        b.thetype = ttype
//      }
//      if b.thetype == LexerSkip {
//        continue_outer = true
//        break
//      }
//      if b.thetype != LexerMore {
//        break
//      }
//    }
//
//    if continue_outer {
//      continue
//    }
//    if b.token == nil {
//      b.Virt.Emit()
//    }
//    return b.token
//  }
//
//  return nil
//}
//
//// Instruct the lexer to Skip creating a token for current lexer rule
//// and look for another token. NextToken() knows to keep looking when
//// a lexer rule finishes with token set to SKIPTOKEN. Recall that
//// if token==nil at end of any token rule, it creates one for you
//// and emits it.
//// /
//pub fn Skip(&self, ) {
//  b.thetype = LexerSkip
//}
//
//pub fn More(&self, ) {
//  b.thetype = LexerMore
//}
//
//pub fn set_mode(&self, m: i32) {
//  b.mode = m
//}
//
//pub fn push_mode(&self, m: i32) {
//  if LexerATNSimulatorDebug {
//    fmt.Println("pushMode " + strconv.Itoa(m))
//  }
//  b.mode_stack.Push(b.mode)
//  b.mode = m
//}
//
//pub fn pop_mode(&self, ) -> i32 {
//  if len(b.mode_stack) == 0 {
//    panic("Empty Stack")
//  }
//  if LexerATNSimulatorDebug {
//    fmt.Println("popMode back to " + fmt.Sprint(b.mode_stack[0:len(b.mode_stack)-1]))
//  }
//  i, _let  = b.mode_stack.Pop();
//  b.mode = i
//  return b.mode
//}
//
//pub fn input_stream(&self, ) -> CharStream {
//  return b.input
//}
//
//pub fn set_input_stream(&self, input: CharStream) {
//  b.input = nil
//  b.token_factory_source_pair = &TokenSourceCharStreamPair{b, b.input}
//  b.reset()
//  b.input = input
//  b.token_factory_source_pair = &TokenSourceCharStreamPair{b, b.input}
//}
//
//// By default does not support multiple emits per NextToken invocation
//// for efficiency reasons. Subclass and override l method, NextToken,
//// and GetToken (to push tokens i32o a list and pull from that list
//// rather than a single variable as l implementation does).
//// /
//pub fn EmitToken(&self, token: Token) {
//  b.token = token
//}
//
//// The standard method called to automatically emit a token at the
//// outermost lexical rule. The token object should point i32o the
//// char buffer start..stop. If there is a text override in 'text',
//// use that to set the token's text. Override l method to emit
//// custom Token objects or provide a Newfactory.
//// /
//pub fn Emit(&self, ) -> Token {
//  let t = b.factory.Create(b.token_factory_source_pair, b.thetype, b.text, b.channel, b.TokenStartCharIndex, b.GetCharIndex()-1, b.TokenStartLine, b.TokenStartColumn);
//  b.EmitToken(t)
//  return t
//}
//
//pub fn EmitEOF(&self, ) -> Token {
//  let cpos = b.GetCharPositionInLine();
//  let lpos = b.GetLine();
//  let eof = b.factory.Create(b.token_factory_source_pair, TokenEOF, "", TokenDefaultChannel, b.input.Index(), b.input.Index()-1, lpos, cpos);
//  b.EmitToken(eof)
//  return eof
//}
//
//pub fn GetCharPositionInLine(&self, ) -> i32 {
//  return b.Interpreter.GetCharPositionInLine()
//}
//
//pub fn GetLine(&self, ) -> i32 {
//  return b.Interpreter.GetLine()
//}
//
//pub fn GetType(&self, ) -> i32 {
//  return b.thetype
//}
//
//pub fn set_type(&self, t: i32) {
//  b.thetype = t
//}
//
//// What is the index of the current character of lookahead?///
//pub fn GetCharIndex(&self, ) -> i32 {
//  return b.input.Index()
//}
//
//// Return the text Matched so far for the current token or any text override.
////Set the complete text of l token it wipes any previous changes to the text.
//pub fn GetText(&self, ) -> &str {
//  if b.text != "" {
//    return b.text
//  }
//
//  return b.Interpreter.GetText(b.input)
//}
//
//pub fn SetText(&self, text: &str) {
//  b.text = text
//}
//
//pub fn GetATN(&self, ) -> *ATN {
//  return b.Interpreter.ATN()
//}
//
//// Return a list of all Token objects in input char stream.
//// Forces load of all tokens. Does not include EOF token.
//// /
//pub fn get_all_tokens(&self, ) -> []Token {
//  let vl = b.Virt;
//  let tokens = make([]Token, 0);
//  let t = vl.NextToken();
//  for t.GetTokenType() != TokenEOF {
//    tokens = append(tokens, t)
//    t = vl.NextToken()
//  }
//  return tokens
//}
//
//pub fn notify_listeners(&self, e: RecognitionException) {
//  let start = b.TokenStartCharIndex;
//  let stop = b.input.Index();
//  let text = b.input.GetTextFromInterval(NewInterval(start, stop));
//  let msg = "token recognition error at: '" + text + "'";
//  let listener = b.GetErrorListenerDispatch();
//  listener.SyntaxError(b, nil, b.TokenStartLine, b.TokenStartColumn, msg, e)
//}
//
//pub fn get_error_display_for_char(&self, c: rune) -> &str {
//  if c == TokenEOF {
//    return "<EOF>"
//  } else if c == '\n' {
//    return "\\n"
//  } else if c == '\t' {
//    return "\\t"
//  } else if c == '\r' {
//    return "\\r"
//  } else {
//    return &str(c)
//  }
//}
//
//pub fn get_char_error_display(&self, c: rune) -> &str {
//  return "'" + b.get_error_display_for_char(c) + "'"
//}
//
//// Lexers can normally Match any char in it's vocabulary after Matching
//// a token, so do the easy thing and just kill a character and hope
//// it all works out. You can instead use the rule invocation stack
//// to do sophisticated error recovery if you are in a fragment rule.
//// /
//pub fn Recover(&self, re: RecognitionException) {
//  if b.input.LA(1) != TokenEOF {
//    if _, let ok = re.(*LexerNoViableAltException); ok {;
//      // Skip a char and try again
//      b.Interpreter.Consume(b.input)
//    } else {
//      // TODO: Do we lose character or line position information?
//      b.input.Consume()
//    }
//  }
//}
