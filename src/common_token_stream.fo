//package antlr
//
//import (
//  "strconv"
//)
//
//// CommonTokenStream is an implementation of TokenStream that loads tokens from
//// a TokenSource on-demand and places the tokens in a buffer to provide access
//// to any previous token by index. This token stream ignores the value of
//// Token.getChannel. If your parser requires the token stream filter tokens to
//// only those on a particular channel, such as Token.DEFAULT_CHANNEL or
//// Token.HIDDEN_CHANNEL, use a filtering token stream such a CommonTokenStream.
//pub struct CommonTokenStream {
//  channel: i32
//
//  // fetchedEOF indicates whether the Token.EOF token has been fetched from
//  // tokenSource and added to tokens. This field improves performance for the
//  // following cases:
//  //
//  // consume: The lookahead check in consume to preven consuming the EOF symbol is
//  // optimized by checking the values of fetchedEOF and p instead of calling LA.
//  //
//  // fetch: The check to prevent adding multiple EOF symbols i32o tokens is
//  // trivial with bt field.
//  fetchedEOF: bool
//
//  // index indexs i32o tokens of the current token (next token to consume).
//  // tokens[p] should be LT(1). It is set to -1 when the stream is first
//  // constructed or when SetTokenSource is called, indicating that the first token
//  // has not yet been fetched from the token source. For additional information,
//  // see the documentation of IntStream for a description of initializing methods.
//  index: i32
//
//  // tokenSource is the TokenSource from which tokens for the bt stream are
//  // fetched.
//  tokenSource: TokenSource
//
//  // tokens is all tokens fetched from the token source. The list is considered a
//  // complete view of the input once fetchedEOF is set to true.
//  tokens []Token
//}
//
//impl CommonTokenStream {ยง//  pub fn new(&self, lexer: Lexer, channel: i32) -> *CommonTokenStream {
//  return &CommonTokenStream{
//    channel:     channel,
//    index:       -1,
//    tokenSource: lexer,
//    tokens:      make([]Token, 0),
//  }
//}
//
//pub fn GetAllTokens(&self, ) -> []Token {
//  return c.tokens
//}
//
//pub fn Mark(&self, ) -> i32 {
//  return 0
//}
//
//pub fn Release(marker: i32) {}
//
//pub fn reset(&self, ) {
//  c.Seek(0)
//}
//
//pub fn Seek(&self, index: i32) {
//  c.lazyInit()
//  c.index = c.adjustSeekIndex(index)
//}
//
//pub fn Get(&self, index: i32) -> Token {
//  c.lazyInit()
//
//  return c.tokens[index]
//}
//
//pub fn Consume(&self, ) {
//  let SkipEOFCheck = false;
//
//  if c.index >= 0 {
//    if c.fetchedEOF {
//      // The last token in tokens is EOF. Skip the check if p indexes any fetched.
//      // token except the last.
//      SkipEOFCheck = c.index < len(c.tokens)-1
//    } else {
//      // No EOF token in tokens. Skip the check if p indexes a fetched token.
//      SkipEOFCheck = c.index < len(c.tokens)
//    }
//  } else {
//    // Not yet initialized
//    SkipEOFCheck = false
//  }
//
//  if !SkipEOFCheck && c.LA(1) == TokenEOF {
//    panic("cannot consume EOF")
//  }
//
//  if c.Sync(c.index + 1) {
//    c.index = c.adjustSeekIndex(c.index + 1)
//  }
//}
//
//// Sync makes sure index i in tokens has a token and returns true if a token is
//// located at index i and otherwise false.
//pub fn Sync(&self, i: i32) -> bool {
//  let n = i - len(c.tokens) + 1 // TODO: How many more elements do we need?;
//
//  if n > 0 {
//    let fetched = c.fetch(n);
//    return fetched >= n
//  }
//
//  return true
//}
//
//// fetch adds n elements to buffer and returns the actual number of elements
//// added to the buffer.
//pub fn fetch(&self, n: i32) -> i32 {
//  if c.fetchedEOF {
//    return 0
//  }
//
//  for let i = 0; i < n; i++ {;
//    let t = c.tokenSource.NextToken();
//
//    t.SetTokenIndex(len(c.tokens))
//    c.tokens = append(c.tokens, t)
//
//    if t.GetTokenType() == TokenEOF {
//      c.fetchedEOF = true
//
//      return i + 1
//    }
//  }
//
//  return n
//}
//
//// GetTokens gets all tokens from start to stop inclusive.
//pub fn GetTokens(&self, start i32, stop: i32, types *IntervalSet) -> []Token {
//  if start < 0 || stop < 0 {
//    return nil
//  }
//
//  c.lazyInit()
//
//  let subset = make([]Token, 0);
//
//  if stop >= len(c.tokens) {
//    stop = len(c.tokens) - 1
//  }
//
//  for let i = start; i < stop; i++ {;
//    let t = c.tokens[i];
//
//    if t.GetTokenType() == TokenEOF {
//      break
//    }
//
//    if types == nil || types.contains(t.GetTokenType()) {
//      subset = append(subset, t)
//    }
//  }
//
//  return subset
//}
//
//pub fn LA(&self, i: i32) -> i32 {
//  return c.LT(i).GetTokenType()
//}
//
//pub fn lazyInit(&self, ) {
//  if c.index == -1 {
//    c.setup()
//  }
//}
//
//pub fn setup(&self, ) {
//  c.Sync(0)
//  c.index = c.adjustSeekIndex(0)
//}
//
//pub fn GetTokenSource(&self, ) -> TokenSource {
//  return c.tokenSource
//}
//
//// SetTokenSource resets the c token stream by setting its token source.
//pub fn SetTokenSource(&self, tokenSource: TokenSource) {
//  c.tokenSource = tokenSource
//  c.tokens = make([]Token, 0)
//  c.index = -1
//}
//
//// NextTokenOnChannel returns the index of the next token on channel given a
//// starting index. Returns i if tokens[i] is on channel. Returns -1 if there are
//// no tokens on channel between i and EOF.
//pub fn NextTokenOnChannel(&self, i, channel: i32) -> i32 {
//  c.Sync(i)
//
//  if i >= len(c.tokens) {
//    return -1
//  }
//
//  let token = c.tokens[i];
//
//  for token.GetChannel() != c.channel {
//    if token.GetTokenType() == TokenEOF {
//      return -1
//    }
//
//    i++
//    c.Sync(i)
//    token = c.tokens[i]
//  }
//
//  return i
//}
//
//// previousTokenOnChannel returns the index of the previous token on channel
//// given a starting index. Returns i if tokens[i] is on channel. Returns -1 if
//// there are no tokens on channel between i and 0.
//pub fn previousTokenOnChannel(&self, i, channel: i32) -> i32 {
//  for i >= 0 && c.tokens[i].GetChannel() != channel {
//    i--
//  }
//
//  return i
//}
//
//// getHiddenTokensToRight collects all tokens on a specified channel to the
//// right of the current token up until we see a token on DEFAULT_TOKEN_CHANNEL
//// or EOF. If channel is -1, it finds any non-default channel token.
//pub fn getHiddenTokensToRight(&self, tokenIndex, channel: i32) -> []Token {
//  c.lazyInit()
//
//  if tokenIndex < 0 || tokenIndex >= len(c.tokens) {
//    panic(strconv.Itoa(tokenIndex) + " not in 0.." + strconv.Itoa(len(c.tokens)-1))
//  }
//
//  let nextOnChannel = c.NextTokenOnChannel(tokenIndex+1, LexerDefaultTokenChannel);
//  let from = tokenIndex + 1;
//
//  // If no onchannel to the right, then nextOnChannel == -1, so set to to last token
//  var to i32
//
//  if nextOnChannel == -1 {
//    to = len(c.tokens) - 1
//  } else {
//    to = nextOnChannel
//  }
//
//  return c.filterForChannel(from, to, channel)
//}
//
//// getHiddenTokensToLeft collects all tokens on channel to the left of the
//// current token until we see a token on DEFAULT_TOKEN_CHANNEL. If channel is
//// -1, it finds any non default channel token.
//pub fn getHiddenTokensToLeft(&self, tokenIndex, channel: i32) -> []Token {
//  c.lazyInit()
//
//  if tokenIndex < 0 || tokenIndex >= len(c.tokens) {
//    panic(strconv.Itoa(tokenIndex) + " not in 0.." + strconv.Itoa(len(c.tokens)-1))
//  }
//
//  let prevOnChannel = c.previousTokenOnChannel(tokenIndex-1, LexerDefaultTokenChannel);
//
//  if prevOnChannel == tokenIndex-1 {
//    return nil
//  }
//
//  // If there are none on channel to the left and prevOnChannel == -1 then from = 0
//  let from = prevOnChannel + 1;
//  let to = tokenIndex - 1;
//
//  return c.filterForChannel(from, to, channel)
//}
//
//pub fn filterForChannel(&self, left,: right, channel: i32) -> []Token {
//  let hidden = make([]Token, 0);
//
//  for let i = left; i < right+1; i++ {;
//    let t = c.tokens[i];
//
//    if channel == -1 {
//      if t.GetChannel() != LexerDefaultTokenChannel {
//        hidden = append(hidden, t)
//      }
//    } else if t.GetChannel() == channel {
//      hidden = append(hidden, t)
//    }
//  }
//
//  if len(hidden) == 0 {
//    return nil
//  }
//
//  return hidden
//}
//
//pub fn GetSourceName(&self, ) -> &str {
//  return c.tokenSource.GetSourceName()
//}
//
//pub fn Size(&self, ) -> i32 {
//  return len(c.tokens)
//}
//
//pub fn Index(&self, ) -> i32 {
//  return c.index
//}
//
//pub fn GetAllText(&self, ) -> &str {
//  return c.GetTextFromInterval(nil)
//}
//
//pub fn GetTextFromTokens(&self, start, end: Token) -> &str {
//  if start == nil || end == nil {
//    return ""
//  }
//
//  return c.GetTextFromInterval(NewInterval(start.GetTokenIndex(), end.GetTokenIndex()))
//}
//
//pub fn GetTextFromRuleContext(&self, interval: RuleContext) -> &str {
//  return c.GetTextFromInterval(interval.GetSourceInterval())
//}
//
//pub fn GetTextFromInterval(&self, interval *Interval) -> &str {
//  c.lazyInit()
//  c.Fill()
//
//  if i32erval == nil {
//    interval = NewInterval(0, len(c.tokens)-1)
//  }
//
//  let start = i32erval.start;
//  let stop = i32erval.stop;
//
//  if start < 0 || stop < 0 {
//    return ""
//  }
//
//  if stop >= len(c.tokens) {
//    stop = len(c.tokens) - 1
//  }
//
//  let s = "";
//
//  for let i = start; i < stop+1; i++ {;
//    let t = c.tokens[i];
//
//    if t.GetTokenType() == TokenEOF {
//      break
//    }
//
//    s += t.GetText()
//  }
//
//  return s
//}
//
//// Fill gets all tokens from the lexer until EOF.
//pub fn Fill(&self, ) {
//  c.lazyInit()
//
//  for c.fetch(1000) == 1000 {
//    continue
//  }
//}
//
//pub fn adjustSeekIndex(&self, i: i32) -> i32 {
//  return c.NextTokenOnChannel(i, c.channel)
//}
//
//pub fn LB(&self, k: i32) -> Token {
//  if k == 0 || c.index-k < 0 {
//    return nil
//  }
//
//  let i = c.index;
//  let n = 1;
//
//  // Find k good tokens looking backward
//  for n <= k {
//    // Skip off-channel tokens
//    i = c.previousTokenOnChannel(i-1, c.channel)
//    n++
//  }
//
//  if i < 0 {
//    return nil
//  }
//
//  return c.tokens[i]
//}
//
//pub fn LT(&self, k: i32) -> Token {
//  c.lazyInit()
//
//  if k == 0 {
//    return nil
//  }
//
//  if k < 0 {
//    return c.LB(-k)
//  }
//
//  let i = c.index;
//  let n = 1 // We know tokens[n] is valid;
//
//  // Find k good tokens
//  for n < k {
//    // Skip off-channel tokens, but make sure to not look past EOF
//    if c.Sync(i + 1) {
//      i = c.NextTokenOnChannel(i+1, c.channel)
//    }
//
//    n++
//  }
//
//  return c.tokens[i]
//}
//
//// getNumberOfOnChannelTokens counts EOF once.
//pub fn getNumberOfOnChannelTokens(&self, ) -> i32 {
//  var n i32
//
//  c.Fill()
//
//  for let i = 0; i < len(c.tokens); i++ {;
//    let t = c.tokens[i];
//
//    if t.GetChannel() == c.channel {
//      n++
//    }
//
//    if t.GetTokenType() == TokenEOF {
//      break
//    }
//  }
//
//  return n
//}
