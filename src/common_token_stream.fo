//package antlr
//
//import (
//  "strconv"
//)
//
//// CommonTokenStream is an implementation of TokenStream that loads tokens from
//// a TokenSource on-demand and places the tokens in a buffer to provide access
//// to any previous token by index. This token stream ignores the value of
//// Token.channel. If your parser requires the token stream filter tokens to
//// only those on a particular channel, such as Token.d_e_f_a_u_lT__c_h_a_nNEL or
//// Token.h_i_d_d_e_n__c_h_a_nNEL, use a filtering token stream such a CommonTokenStream.
//pub struct CommonTokenStream {
//  channel: i32
//
//  // fetched_e_o_f indicates whether the Token.e_o_f token has been fetched from
//  // token_source and added to tokens. This field improves performance for the
//  // following cases:
//  //
//  // consume: The lookahead check in consume to preven consuming the EOF symbol is
//  // optimized by checking the values of fetched_e_o_f and p instead of calling LA.
//  //
//  // fetch: The check to prevent adding multiple EOF symbols i32o tokens is
//  // trivial with bt field.
//  fetched_e_o_f: bool
//
//  // index indexs i32o tokens of the current token (next token to consume).
//  // tokens[p] should be LT(1). It is set to -1 when the stream is first
//  // constructed or when SetTokenSource is called, indicating that the first token
//  // has not yet been fetched from the token source. For additional information,
//  // see the documentation of IntStream for a description of initializing methods.
//  index: i32
//
//  // token_source is the TokenSource from which tokens for the bt stream are
//  // fetched.
//  token_source: TokenSource
//
//  // tokens is all tokens fetched from the token source. The list is considered a
//  // complete view of the input once fetched_e_o_f is set to true.
//  tokens []Token
//}
//
//impl CommonTokenStream {ยง//  fn new(&self, lexer: Lexer, channel: i32) -> *CommonTokenStream {
//  return &CommonTokenStream{
//    channel:     channel,
//    index:       -1,
//    token_source: lexer,
//    tokens:      make([]Token, 0),
//  }
//}
//
//pub fn all_tokens(&self) -> []Token {
//  return self.tokens
//}
//
//pub fn mark(&self) -> i32 {
//  return 0
//}
//
//pub fn release(marker: i32) {}
//
//fn reset(&self) {
//  self.seek(0)
//}
//
//pub fn seek(&self, index: i32) {
//  self.lazy_init()
//  self.index = self.adjust_seek_index(index)
//}
//
//pub fn get(&self, index: i32) -> Token {
//  self.lazy_init()
//
//  return self.tokens[index]
//}
//
//pub fn consume(&self) {
//  let SkipEOFCheck = false;
//
//  if self.index >= 0 {
//    if self.fetched_e_o_f {
//      // The last token in tokens is EOF. Skip the check if p indexes any fetched.
//      // token except the last.
//      SkipEOFCheck = self.index < len(self.tokens)-1
//    } else {
//      // No EOF token in tokens. Skip the check if p indexes a fetched token.
//      SkipEOFCheck = self.index < len(self.tokens)
//    }
//  } else {
//    // Not yet initialized
//    SkipEOFCheck = false
//  }
//
//  if !SkipEOFCheck && self.l_a(1) == TokenEOF {
//    panic!("cannot consume EOF")
//  }
//
//  if self.sync(self.index + 1) {
//    self.index = self.adjust_seek_index(self.index + 1)
//  }
//}
//
//// Sync makes sure index i in tokens has a token and returns true if a token is
//// located at index i and otherwise false.
//pub fn sync(&self, i: i32) -> bool {
//  let n = i - len(self.tokens) + 1 // TODO: How many more elements do we need?;
//
//  if n > 0 {
//    let fetched = self.fetch(n);
//    return fetched >= n
//  }
//
//  return true
//}
//
//// fetch adds n elements to buffer and returns the actual number of elements
//// added to the buffer.
//fn fetch(&self, n: i32) -> i32 {
//  if self.fetched_e_o_f {
//    return 0
//  }
//
//  for let i = 0; i < n; i++ {;
//    let t = self.token_source.next_token();
//
//    self.set_token_index(len(self.tokens))
//    self.tokens = append(self.tokens, t)
//
//    if self.token_type() == TokenEOF {
//      self.fetched_e_o_f = true
//
//      return i + 1
//    }
//  }
//
//  return n
//}
//
//// GetTokens gets all tokens from start to stop inclusive.
//pub fn tokens(&self, start i32, stop: i32, types *IntervalSet) -> []Token {
//  if start < 0 || stop < 0 {
//    return nil
//  }
//
//  self.lazy_init()
//
//  let subset = make([]Token, 0);
//
//  if stop >= len(self.tokens) {
//    stop = len(self.tokens) - 1
//  }
//
//  for let i = start; i < stop; i++ {;
//    let t = self.tokens[i];
//
//    if self.token_type() == TokenEOF {
//      break
//    }
//
//    if types == nil || types.contains(self.token_type()) {
//      subset = append(subset, t)
//    }
//  }
//
//  return subset
//}
//
//pub fn l_a(&self, i: i32) -> i32 {
//  return self.l_t(i).token_type()
//}
//
//fn lazy_init(&self) {
//  if self.index == -1 {
//    self.setup()
//  }
//}
//
//fn setup(&self) {
//  self.sync(0)
//  self.index = self.adjust_seek_index(0)
//}
//
//pub fn token_source(&self) -> TokenSource {
//  return self.token_source
//}
//
//// SetTokenSource resets the c token stream by setting its token source.
//pub fn set_token_source(&self, token_source: TokenSource) {
//  self.token_source = token_source
//  self.tokens = make([]Token, 0)
//  self.index = -1
//}
//
//// NextTokenOnChannel returns the index of the next token on channel given a
//// starting index. Returns i if tokens[i] is on channel. Returns -1 if there are
//// no tokens on channel between i and EOF.
//pub fn next_token_on_channel(&self, i, channel: i32) -> i32 {
//  self.sync(i)
//
//  if i >= len(self.tokens) {
//    return -1
//  }
//
//  let token = self.tokens[i];
//
//  for token.channel() != self.channel {
//    if token.token_type() == TokenEOF {
//      return -1
//    }
//
//    i++
//    self.sync(i)
//    token = self.tokens[i]
//  }
//
//  return i
//}
//
//// previous_token_on_channel returns the index of the previous token on channel
//// given a starting index. Returns i if tokens[i] is on channel. Returns -1 if
//// there are no tokens on channel between i and 0.
//fn previous_token_on_channel(&self, i, channel: i32) -> i32 {
//  for i >= 0 && self.tokens[i].channel() != channel {
//    i--
//  }
//
//  return i
//}
//
//// hidden_tokens_to_right collects all tokens on a specified channel to the
//// right of the current token up until we see a token on DEFAULT__t_o_k_eN__c_h_a_nNEL
//// or EOF. If channel is -1, it finds any non-default channel token.
//fn hidden_tokens_to_right(&self, token_index, channel: i32) -> []Token {
//  self.lazy_init()
//
//  if token_index < 0 || token_index >= len(self.tokens) {
//    panic!(strconv.itoa(tokenIndex) + " not in 0.." + strconv.itoa(len(self.tokens)-1))
//  }
//
//  let next_on_channel = self.next_token_on_channel(tokenIndex+1, LexerDefaultTokenChannel);
//  let from = token_index + 1;
//
//  // If no onchannel to the right, then next_on_channel == -1, so set to to last token
//  var to i32
//
//  if next_on_channel == -1 {
//    to = len(self.tokens) - 1
//  } else {
//    to = next_on_channel
//  }
//
//  return self.filter_for_channel(from, to, channel)
//}
//
//// hidden_tokens_to_left collects all tokens on channel to the left of the
//// current token until we see a token on DEFAULT__t_o_k_eN__c_h_a_nNEL. If channel is
//// -1, it finds any non default channel token.
//fn hidden_tokens_to_left(&self, token_index, channel: i32) -> []Token {
//  self.lazy_init()
//
//  if token_index < 0 || token_index >= len(self.tokens) {
//    panic!(strconv.itoa(tokenIndex) + " not in 0.." + strconv.itoa(len(self.tokens)-1))
//  }
//
//  let prev_on_channel = self.previous_token_on_channel(tokenIndex-1, LexerDefaultTokenChannel);
//
//  if prev_on_channel == token_index-1 {
//    return nil
//  }
//
//  // If there are none on channel to the left and prev_on_channel == -1 then from = 0
//  let from = prev_on_channel + 1;
//  let to = token_index - 1;
//
//  return self.filter_for_channel(from, to, channel)
//}
//
//fn filter_for_channel(&self, left,: right, channel: i32) -> []Token {
//  let hidden = make([]Token, 0);
//
//  for let i = left; i < right+1; i++ {;
//    let t = self.tokens[i];
//
//    if channel == -1 {
//      if self.channel() != LexerDefaultTokenChannel {
//        hidden = append(hidden, t)
//      }
//    } else if self.channel() == channel {
//      hidden = append(hidden, t)
//    }
//  }
//
//  if len(hidden) == 0 {
//    return nil
//  }
//
//  return hidden
//}
//
//pub fn source_name(&self) -> &str {
//  return self.token_source.source_name()
//}
//
//pub fn size(&self) -> i32 {
//  return len(self.tokens)
//}
//
//pub fn index(&self) -> i32 {
//  return self.index
//}
//
//pub fn all_text(&self) -> &str {
//  return self.text_from_interval(nil)
//}
//
//pub fn text_from_tokens(&self, start, end: Token) -> &str {
//  if start == nil || end == nil {
//    return ""
//  }
//
//  return self.text_from_interval(NewInterval(start.token_index(), end.token_index()))
//}
//
//pub fn text_from_rule_context(&self, interval: RuleContext) -> &str {
//  return self.text_from_interval(interval.source_interval())
//}
//
//pub fn text_from_interval(&self, interval *Interval) -> &str {
//  self.lazy_init()
//  self.fill()
//
//  if i32erval == nil {
//    interval = Interval::new(0, len(self.tokens)-1)
//  }
//
//  let start = i32erval.start;
//  let stop = i32erval.stop;
//
//  if start < 0 || stop < 0 {
//    return ""
//  }
//
//  if stop >= len(self.tokens) {
//    stop = len(self.tokens) - 1
//  }
//
//  let s = "";
//
//  for let i = start; i < stop+1; i++ {;
//    let t = self.tokens[i];
//
//    if self.token_type() == TokenEOF {
//      break
//    }
//
//    s += self.text()
//  }
//
//  return s
//}
//
//// Fill gets all tokens from the lexer until EOF.
//pub fn fill(&self) {
//  self.lazy_init()
//
//  for self.fetch(1000) == 1000 {
//    continue
//  }
//}
//
//fn adjust_seek_index(&self, i: i32) -> i32 {
//  return self.next_token_on_channel(i, self.channel)
//}
//
//pub fn l_b(&self, k: i32) -> Token {
//  if k == 0 || self.index-k < 0 {
//    return nil
//  }
//
//  let i = self.index;
//  let n = 1;
//
//  // Find k good tokens looking backward
//  for n <= k {
//    // Skip off-channel tokens
//    i = self.previous_token_on_channel(i-1, self.channel)
//    n++
//  }
//
//  if i < 0 {
//    return nil
//  }
//
//  return self.tokens[i]
//}
//
//pub fn l_t(&self, k: i32) -> Token {
//  self.lazy_init()
//
//  if k == 0 {
//    return nil
//  }
//
//  if k < 0 {
//    return self.l_b(-k)
//  }
//
//  let i = self.index;
//  let n = 1 // We know tokens[n] is valid;
//
//  // Find k good tokens
//  for n < k {
//    // Skip off-channel tokens, but make sure to not look past EOF
//    if self.sync(i + 1) {
//      i = self.next_token_on_channel(i+1, self.channel)
//    }
//
//    n++
//  }
//
//  return self.tokens[i]
//}
//
//// number_of_on_channel_tokens counts EOF once.
//fn number_of_on_channel_tokens(&self) -> i32 {
//  var n i32
//
//  self.fill()
//
//  for let i = 0; i < len(self.tokens); i++ {;
//    let t = self.tokens[i];
//
//    if self.channel() == self.channel {
//      n++
//    }
//
//    if self.token_type() == TokenEOF {
//      break
//    }
//  }
//
//  return n
//}
